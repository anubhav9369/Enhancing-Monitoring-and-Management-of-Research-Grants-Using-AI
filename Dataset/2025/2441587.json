{
 "awd_id": "2441587",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Enhancing Adaptability in Multimodal Human-Robot Interaction",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924420",
 "po_email": "cbethel@nsf.gov",
 "po_sign_block_name": "Cindy Bethel",
 "awd_eff_date": "2025-06-15",
 "awd_exp_date": "2030-05-31",
 "tot_intn_awd_amt": 630000.0,
 "awd_amount": 377674.0,
 "awd_min_amd_letter_date": "2025-06-10",
 "awd_max_amd_letter_date": "2025-06-10",
 "awd_abstract_narration": "As robots become increasingly helpful in factories, hospitals, and homes, they must learn how to interact effectively with people. When people communicate, they use words and body language\u2014like hand movements and eye contact\u2014to share what they mean. In the same way, robots that work closely with people need to understand both verbal words and nonverbal signals. One major challenge in creating such robots is that people\u2019s behavior can change over time. For example, a person may use different gestures (e.g., pointing vs eye gaze) to describe an object at different times. The robot must notice these changes and adjust its responses. Also, if the surroundings change, robots must change how they act to keep being helpful. This project will help robots understand people better by studying both verbal messages and nonverbal cues. It will also improve how robots learn and plan to adjust when people\u2019s behavior changes. The results of this research will impact people and help them to be more accepting of robots. The project will help students in several ways. First, new topics will be added to robotics classes. Second, the research will involve high school and college students. Finally, the team will share robotic advancements with the public through outreach activities. \r\n\r\n\r\nThis project will focus on three goals to address challenges in multimodal human-robot interactions. First, it will develop advanced algorithms to understand human intentions. These algorithms will use multimodal data, such as verbal messages, hand gestures, eye gaze, and head movements. Second, the project will develop new learning and planning methods for robots. These methods will help robots plan and act in ways that align with human intentions. Lastly, the research will design adaptive learning techniques. These techniques help robots adapt to changes in human behavior to support long-term interactions. The proposed system will be validated through several comprehensive human-robot interaction studies. This project will move the HRI field forward by showing how we can create effective and adaptable long-term interactions between humans and robots. The results of this research will impact how people perceive and accept robots that will be more prevalent in their lives.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Tariq",
   "pi_last_name": "Iqbal",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Tariq Iqbal",
   "pi_email_addr": "ti3fe@virginia.edu",
   "nsf_id": "000812858",
   "pi_start_date": "2025-06-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Virginia Main Campus",
  "inst_street_address": "1001 EMMET ST N",
  "inst_street_address_2": "",
  "inst_city_name": "CHARLOTTESVILLE",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "4349244270",
  "inst_zip_code": "229034833",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "VA05",
  "org_lgl_bus_name": "RECTOR & VISITORS OF THE UNIVERSITY OF VIRGINIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "JJG6HU8PA4S5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Virginia Main Campus",
  "perf_str_addr": "1001 EMMET ST N",
  "perf_city_name": "CHARLOTTESVILLE",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "229034833",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "VA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002526DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002829DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2025,
   "fund_oblg_amt": 377674.0
  }
 ],
 "por": null
}