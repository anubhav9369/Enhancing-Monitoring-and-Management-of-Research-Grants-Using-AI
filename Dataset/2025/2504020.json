{
 "awd_id": "2504020",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: RI: Medium: Using Systematic Relationships and Phonetic Speech Foundation Models for Universal Speech-to-Text across Varieties of Languages",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032922972",
 "po_email": "emiltsak@nsf.gov",
 "po_sign_block_name": "Eleni Miltsakaki",
 "awd_eff_date": "2025-09-01",
 "awd_exp_date": "2029-08-31",
 "tot_intn_awd_amt": 377764.0,
 "awd_amount": 377764.0,
 "awd_min_amd_letter_date": "2025-06-23",
 "awd_max_amd_letter_date": "2025-06-23",
 "awd_abstract_narration": "This project introduces a new way to develop speech-to-text systems for language varieties in which little digital data is available, especially when a related variety already has plenty of digital data. Languages differ from one another, but they also show a great deal of variation within themselves. People in different regions or countries often pronounce words in the same language differently. For example, many people in the southern US pronounce the \u201ci\u201d in words like \u201cride\u201d as an \u201ca\u201d sound (similar to the one found in \u201crad\u201d), while most other Americans do not. Most speech-to-text systems\u2014which turn spoken words into written text\u2014focus on just one variety of each language. However, in everyday life, many people speak other varieties. Existing speech-to-text systems often do not work well for these varieties. Improving speech recognition for low-resource varieties represents a business opportunity, one that can help more Americans access voice-powered tools and services. This project takes one step towards this goal. It innovates by leveraging the fact that the differences between various varieties of the same language often follow predictable patterns. For example, since the pronunciations of words in different regions change following rules that apply to the whole vocabulary, one can often predict how a word will be pronounced in one variety if one knows how it is pronounced in another. The project will develop a powerful AI model (POWSM) that can transcribe pronunciation (using a universal system developed by linguists to represent sounds). This approach enables the development of speech-to-text systems for previously unsupported language variants, even when little recorded training data exists for them. It works by learning both the similarities and the systematic differences between well-resourced varieties and others.\r\n\r\nThe project builds an encoder-decoder foundation speech model called POWSM (Phonetic Open Whisper-style Speech Model), which is trained to recognize speech as sequences of phones (consonants and vowels) in any language. The project will include three applications using this model: 1) Prompting POWSM with vector representations of the systematic sound correspondences between a low-resource variety (LRV) and a high-resource variety (HRV), enabling the model to recognize the LRV as a transformed variant of the HRV. 2) Constructing stochastic weighted finite-state transducers that can generate synthetic LRV data based on linguist-curated knowledge about sound changes in the HRV and LRV. Using this synthetic data to train a language model that can be used to decode LRV output from POWSM. 3) Learning phonetic correspondences between language varieties automatically from transcribed audio using a novel form of unsupervised bilingual lexicon induction (UBLI) that leverages both text and audio. Audio aligned in this way can be used to train basic speech-to-speech translation models that, when used in conjunction with POWSM, enable speech technologies for LRVs without requiring linguistic annotation beyond transcription. The proposed methods are intended to cover most HRV-LRV scenarios. They will be evaluated on major languages including varieties in English, Italian, Chinese, German, Arabic, German, and Dutch, English, as well as endangered languages like Nahuatl and Mixtec.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jonathan",
   "pi_last_name": "Amith",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Jonathan D Amith",
   "pi_email_addr": "jamith@gettysburg.edu",
   "nsf_id": "000197819",
   "pi_start_date": "2025-06-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Gettysburg College",
  "inst_street_address": "300 N WASHINGTON ST",
  "inst_street_address_2": "",
  "inst_city_name": "GETTYSBURG",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "7173376505",
  "inst_zip_code": "173251483",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "PA13",
  "org_lgl_bus_name": "GETTYSBURG COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "LM61PK5X7HK9"
 },
 "perf_inst": {
  "perf_inst_name": "Gettysburg College",
  "perf_str_addr": "300 N WASHINGTON ST",
  "perf_city_name": "GETTYSBURG",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "173251400",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "PA13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002526DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2025,
   "fund_oblg_amt": 377764.0
  }
 ],
 "por": null
}